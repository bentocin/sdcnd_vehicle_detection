{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Driving Car Engineer Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Build a Vehicle Detection Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the goal is to write a software pipeline to detect vehicles in a video (start with the test_video.mp4 and later implement on full project_video.mp4).\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n",
    "* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. \n",
    "* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.\n",
    "* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.\n",
    "* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "* Estimate a bounding box for vehicles detected.\n",
    "\n",
    "Here are links to the labeled data for [vehicle](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip) and [non-vehicle](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles.zip) examples to train your classifier.  These example images come from a combination of the [GTI vehicle image database](http://www.gti.ssr.upm.es/data/Vehicle_database.html), the [KITTI vision benchmark suite](http://www.cvlibs.net/datasets/kitti/), and examples extracted from the project video itself.   You are welcome and encouraged to take advantage of the recently released [Udacity labeled dataset](https://github.com/udacity/self-driving-car/tree/master/annotations) to augment your training data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.measurements import label\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "# TODO: Read images with cv2 and be aware of BGR format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# HELPER FUNCTIONS\n",
    "###################################################################\n",
    "\n",
    "# BOUNDING BOXES\n",
    "# Draw bounding boxes on an image\n",
    "def draw_boxes(img, bboxes, color=(255, 0, 0), thick=6):\n",
    "\"\"\"\n",
    "img: image to draw on\n",
    "bboxes: list of tuples with coordinates for top left and bottom right corner\n",
    "color: color of the bounding boxes as RGB tuple\n",
    "thick: thickness of the line of the bounding boxes\n",
    "return: copy of the image with drawn bounding boxes\n",
    "\"\"\"\n",
    "    imcopy = np.copy(img)\n",
    "    \n",
    "    for box in bboxes:\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    return imcopy\n",
    "\n",
    "# PLOT COLOR SPACE IN 3D\n",
    "# Compute 3D scatter plot to visualize image pixels in color space\n",
    "def plot3d(img, colors_rgb, axis_labels=list(\"RGB\"), axis_limits=((0,255), (0,255), (0,255))):\n",
    "\"\"\"\n",
    "img: the image which is to analyze \n",
    "colors_rgb: colors for the pixel values\n",
    "axis_labels: list of label names for the axes\n",
    "axis_limits: tuple with lower and upper limits of the axes\n",
    "return: Axes3D object\n",
    "\"\"\"\n",
    "    # Create figure and 3D axes\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = Axes3D(fig)\n",
    "    \n",
    "    # Set axis limits\n",
    "    ax.set_xlim(*axis_limits[0])\n",
    "    ax.set_ylim(*axis_limits[1])\n",
    "    ax.set_zlim(*axis_limits[2])\n",
    "    \n",
    "    # Set axis labels and sizes\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=14, pad=8)\n",
    "    ax.set_xlabel(axis_labels[0], fontsize=16, labelpad=16)\n",
    "    ax.set_ylabel(axis_labels[1], fontsize=16, labelpad=16)\n",
    "    ax.set_zlabel(axis_labels[2], fontsize=16, labelpad=16)\n",
    "    \n",
    "    # Plot pixel values with colors given in colors_rgb\n",
    "    ax.scatter(\n",
    "        img[:,:,0].ravel(),\n",
    "        img[:,:,1].ravel(),\n",
    "        img[:,:,2].ravel(),\n",
    "        c=colors_rgb.reshape((-1,3)), edgecolors='none')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# FEATURE EXTRACTION\n",
    "###################################################################\n",
    "\n",
    "# Dictionary for color transform mapping on default RGB images\n",
    "RGB_COLOR_TRANSFORMS = {\"HSV\": cv2.COLOR_RGB2HSV, \"HLS\": cv2.COLOR_RGB2HLS,\n",
    "                       \"LUV\": cv2.RGB2LUV, \"YUV\": cv2.RGB2YUV, \"YCrCb\": cv2.COLOR_RGB2YCrCb}\n",
    "BGR_COLOR_TRANSFORMS = {\"RGB\": cv2.COLOR_BGR2RGB, \"HSV\": cv2.COLOR_BGR2HSV, \"HLS\": cv2.COLOR_BGR2HLS,\n",
    "                       \"LUV\": cv2.BGR2LUV, \"YUV\": cv2.BGR2YUV, \"YCrCb\": cv2.COLOR_BGR2YCrCb}\n",
    "# TODO: Maybe add more colorspaces\n",
    "\n",
    "# COLOR HISTOGRAM\n",
    "# Compute color histogram features\n",
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "\"\"\"\n",
    "img: image to compute the color histogram on\n",
    "nbins: number of bins for the histogram\n",
    "bins_range: a tuple with the lower and upper range of the bins\n",
    "return: histogram for each channel of the image, array with centers of the \n",
    "        bins and feature vector of the histograms\n",
    "\"\"\"\n",
    "    channel_histograms = []\n",
    "    # Compute the histogram of each channel separately\n",
    "    for channel in range(img.shape[-1]):\n",
    "        hist = np.histogram(img[:,:,channel], bins=nbins, range=bins_range)\n",
    "        channel_histograms.append(hist)\n",
    "    \n",
    "    # Generating bin centers\n",
    "    bin_edges = channel_histograms[0][1]\n",
    "    bin_centers = (bin_edges[1:] + bin_edges[0:len(bin_edges)-1])/2\n",
    "    \n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    # TODO: concatenate histogram features\n",
    "    # TODO: retunr channel histograms, bin centers and feature vector\n",
    "    return channel_histograms\n",
    "\n",
    "# SPATIAL BINNING\n",
    "# Compute spatial color features\n",
    "def bin_spatial(img, color_space=None, size=(32,32), color_transforms=RGB_COLOR_TRANSFORMS):\n",
    "\"\"\"\n",
    "img: image that should be converted into spatial features\n",
    "color_space: color space to take the features of\n",
    "size: size of spatial feature area\n",
    "return: spatial feature vector\n",
    "\"\"\"\n",
    "    # Convert image to new color space\n",
    "    if color_space != None:\n",
    "        feature_img = cv2.cvtColor(img, color_transforms[color_space])\n",
    "    else:\n",
    "        feature_img = np.copy(img)\n",
    "    # Create spatial color feature vector\n",
    "    features = cv2.resize(feature_img, size).ravel()\n",
    "    \n",
    "    return features\n",
    "    \n",
    "# HOG FEATURES\n",
    "# Compute histogram of oriented gradients (hog) features\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, vis=False, feature_vec=True):\n",
    "\"\"\"\n",
    "img: image where features should be extracted\n",
    "orient: number of different gradient orientations to consider\n",
    "pix_per_cell: size of the cells\n",
    "cell_per_block: number of cells for each block\n",
    "vis: should the hog feature visualization be returned\n",
    "feature_vec: features should be returned as vector\n",
    "return: features or features and an image with a hog feature visualization\n",
    "\"\"\"\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                 cells_per_block=(cell_per_block, cell_per_block), visualize=True, \n",
    "                                 feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    else:\n",
    "        features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                      cells_per_block=(cell_per_block, cell_per_block), visualize=False, feature_vector=feature_vec)\n",
    "        return features\n",
    "\n",
    "# COMBINE\n",
    "# Extract features from a list of images \n",
    "def extract_features(imgs, cspace=None, spatial_size=(32, 32), hist_bins=32, hist_range=(0,256), \n",
    "                     color_transforms=RGB_COLOR_TRANSFORMS):\n",
    "\"\"\"\n",
    "imgs: list of images to extract features on\n",
    "cspace: colorspace on which to operate\n",
    "spatial_size: size for the spatial binning\n",
    "hist_bins: number of bins for color histograms\n",
    "hist_range: range of the histograms\n",
    "return: list of feature vectors\n",
    "\"\"\"\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    \n",
    "    # Transform colorspace if necessary\n",
    "    for img in imgs:\n",
    "        if cspace != None:\n",
    "            feature_img = cv2.cvtColor(img, color_transforms[cspace])\n",
    "        else:\n",
    "            feature_img = np.copy(img)\n",
    "        # Apply spatial binning\n",
    "        bin_features = bin_spatial(feature_img, size=spatial_size)\n",
    "        # Apply color histogram\n",
    "        hist_features = color_hist(feature_img, nbins=hist_bins, bins_range=hist_range)\n",
    "        # Append the feature vector to the list\n",
    "        features.append(np.concatenate((bin_features, hist_features), axis=0))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# NORMALIZE\n",
    "# \n",
    "def normalize_features(features):\n",
    "\"\"\"\n",
    "features: list of feature vectors\n",
    "return: normalized list of feature vectors\n",
    "\"\"\"\n",
    "    # Fit a per column scaler\n",
    "    X_scaler = StandardScaler().fit(features)\n",
    "    # Apply the scaler to the features\n",
    "    scaled_X = X_scaler.transform(features)\n",
    "    \n",
    "    return scaled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example Code for some of the Functions\n",
    "# Read a color image\n",
    "img = cv2.imread(\"000275.png\")\n",
    "\n",
    "# Select a small fraction of pixels to plot by subsampling it\n",
    "scale = max(img.shape[0], img.shape[1], 64) / 64  # at most 64 rows and columns\n",
    "img_small = cv2.resize(img, (np.int(img.shape[1] / scale), np.int(img.shape[0] / scale)),\n",
    "                       interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# Convert subsampled image to desired color space(s)\n",
    "img_small_RGB = cv2.cvtColor(img_small, cv2.COLOR_BGR2RGB)  # OpenCV uses BGR, matplotlib likes RGB\n",
    "img_small_HSV = cv2.cvtColor(img_small, cv2.COLOR_BGR2HSV)\n",
    "img_small_rgb = img_small_RGB / 255.  # scaled to [0, 1], only for plotting\n",
    "# Plot and show\n",
    "plot3d(img_small_RGB, img_small_rgb)\n",
    "plt.show()\n",
    "\n",
    "plot3d(img_small_HSV, img_small_rgb, axis_labels=list(\"HSV\"))\n",
    "plt.show()\n",
    "\n",
    "# DATA EXPLORATION\n",
    "images = glob.glob('*.jpeg')\n",
    "cars = []\n",
    "notcars = []\n",
    "\n",
    "for image in images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "        \n",
    "# Define a function to return some characteristics of the dataset \n",
    "def data_look(car_list, notcar_list):\n",
    "    data_dict = {}\n",
    "    # Define a key in data_dict \"n_cars\" and store the number of car images\n",
    "    data_dict[\"n_cars\"] = len(car_list)\n",
    "    # Define a key \"n_notcars\" and store the number of notcar images\n",
    "    data_dict[\"n_notcars\"] = len(notcar_list)\n",
    "    # Read in a test image, either car or notcar\n",
    "    img = mpimg.imread(car_list[0])\n",
    "    # Define a key \"image_shape\" and store the test image shape 3-tuple\n",
    "    data_dict[\"image_shape\"] = img.shape\n",
    "    # Define a key \"data_type\" and store the data type of the test image.\n",
    "    data_dict[\"data_type\"] = img.dtype\n",
    "    # Return data_dict\n",
    "    return data_dict\n",
    "    \n",
    "data_info = data_look(cars, notcars)\n",
    "\n",
    "print('Your function returned a count of', \n",
    "      data_info[\"n_cars\"], ' cars and', \n",
    "      data_info[\"n_notcars\"], ' non-cars')\n",
    "print('of size: ',data_info[\"image_shape\"], ' and data type:', \n",
    "      data_info[\"data_type\"])\n",
    "# Just for fun choose random car / not-car indices and plot example images   \n",
    "car_ind = np.random.randint(0, len(cars))\n",
    "notcar_ind = np.random.randint(0, len(notcars))\n",
    "    \n",
    "# Read in car / not-car images\n",
    "car_image = mpimg.imread(cars[car_ind])\n",
    "notcar_image = mpimg.imread(notcars[notcar_ind])\n",
    "\n",
    "\n",
    "# Plot the examples\n",
    "fig = plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(car_image)\n",
    "plt.title('Example Car Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(notcar_image)\n",
    "plt.title('Example Not-car Image')\n",
    "\n",
    "# Create an array stack of feature vectors\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "car_ind = np.random.randint(0, len(cars))\n",
    "\n",
    "# Add heat to each box in box list\n",
    "heat = add_heat(heat,box_list)\n",
    "    \n",
    "# Apply threshold to help remove false positives\n",
    "heat = apply_threshold(heat,1)\n",
    "\n",
    "# Visualize the heatmap when displaying    \n",
    "heatmap = np.clip(heat, 0, 255)\n",
    "\n",
    "# Find final boxes from heatmap using label function\n",
    "labels = label(heatmap)\n",
    "draw_img = draw_labeled_bboxes(np.copy(image), labels)\n",
    "\n",
    "\n",
    "### TODO: Tweak these parameters and see how the results change.\n",
    "color_space = 'HLS' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 12  # HOG orientations\n",
    "pix_per_cell = 8 # HOG pixels per cell\n",
    "cell_per_block = 2 # HOG cells per block\n",
    "hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (16, 16) # Spatial binning dimensions\n",
    "hist_bins = 32    # Number of histogram bins\n",
    "spatial_feat = True # Spatial features on or off\n",
    "hist_feat = False # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off\n",
    "y_start_stop = [400, None] # Min and max in y to search in slide_window()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# CLASSIFIER\n",
    "###################################################################\n",
    "\n",
    "# LINEAR SVM\n",
    "# Build a linear SVM classifier\n",
    "def build_linear_svm(train, parameters):\n",
    "\"\"\"\n",
    "train: tuple of data for training of the classifier\n",
    "parameters: dictionary of parameters for grid search\n",
    "return: classifier and best parameters\n",
    "\"\"\"\n",
    "    svr = svm.SVC()\n",
    "    # Creates the grid of possible parameter combinations\n",
    "    clf = grid_search.GridSearchCV(svr, parameters)\n",
    "    # Tries all parameter combinations and returns the fitted classifier\n",
    "    clf.fit(*train)\n",
    "    return clf, clf.best_params_\n",
    "\n",
    "# TODO: Implement other classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Color Classify Parameters:\n",
    "Spatial = 50\n",
    "Histbin = 32\n",
    "# --> 97.5% Test Accuracy\n",
    "\n",
    "# HOG Classify Parameters:\n",
    "Colorspace = 'HLS'\n",
    "Orient = 12\n",
    "Pix_per_cell = 8\n",
    "Cell_per_block = 2\n",
    "Hog_channel = \"ALL\"\n",
    "# --> 99% Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# OBJECT DETECTION\n",
    "###################################################################\n",
    "\n",
    "# SLIDING WINDOW\n",
    "# Slide a window over the image that takes those sub samples for classification\n",
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                 xy_window=(64,64), xy_overlap=(0.5,0.5)):\n",
    "\"\"\"\n",
    "img: image that should be scanned for objects\n",
    "x_start_stop: start and stop positions for the scanning area in x direction\n",
    "y_start_stop: start and stop positions for the scanning area in y direction\n",
    "xy_window: dimensions of the sliding window\n",
    "xy_overlap: overlap of windows in x and y direction\n",
    "return: list of tuples with window start and end positions\n",
    "\"\"\"\n",
    "    # Set start stop positions if not defined\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    it y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "        \n",
    "    # Compute the span of the region to be searched\n",
    "    x_span = np.int(x_start_stop[1] - x_start_stop[0])\n",
    "    y_span = np.int(y_start_stop[1] - y_start_stop[0])\n",
    "    \n",
    "    # Compute the number of pixels per step in x|y\n",
    "    step_pix_x = xy_window[0] * (1 - xy_overlap[0])\n",
    "    step_pix_y = xy_window[1] * (1 - xy_overlap[1])\n",
    "    \n",
    "    # Compute the number of windows in x|y\n",
    "    x_buffer = np.int(xy_window[0] * xy_overlap[0])\n",
    "    y_buffer = np.int(xy_window[1] * xy_overlap[1])\n",
    "    n_windows_x = np.int((x_span - x_buffer) / step_pix_x)\n",
    "    n_windows_y = np.int((y_span - y_buffer) / step_pix_y)\n",
    "    \n",
    "    # Initialize a list to append window positions\n",
    "    window_list = []\n",
    "    \n",
    "    # Loop through finding x and y window positions\n",
    "    for step_y in range(n_windows_y):\n",
    "        for step_x in range(n_windows_x):\n",
    "            # Calculate each window position\n",
    "            start_x = np.int(step_x * step_pix_x + x_start_stop[0])\n",
    "            start_y = np.int(step_y * step_pix_y + y_start_stop[0])\n",
    "            end_x = np.int(start_x + xy_window[0])\n",
    "            end_y = np.int(start_y + xy_window[1])\n",
    "            # Append position to list\n",
    "            window_list.append(((start_x, start_y), (end_x, end_y)))\n",
    "    return window_list\n",
    "\n",
    "# HOG SUBSAMPLING WINDOW SEARCH\n",
    "# Extract features using hog sub-sampling and make predictions\n",
    "def find_cars(img, y_start_stop, x_start_stop, scale, svc, X_scaler, window=64, orient, pix_per_cell, cell_per_block, \n",
    "              cells_per_step=2, spatial_size, hist_bins, color=(255, 0, 0), thick=6, cspace=None, color_transforms=RGB_COLOR_TRANSFORMS):\n",
    "\"\"\"\n",
    "img:\n",
    "y_start_stop:\n",
    "x_start_stop:\n",
    "scale:\n",
    "svc:\n",
    "X_scaler:\n",
    "window:\n",
    "orient:\n",
    "pix_per_cell:\n",
    "cell_per_block:\n",
    "cells_per_step: instead of overlap define how many cells to step\n",
    "spatial_size:\n",
    "hist_bins:\n",
    "color: color of the bounding box lines\n",
    "thick: thickness of the bounding box lines\n",
    "cspace:\n",
    "color_transforms: \n",
    "return:\n",
    "\"\"\"\n",
    "    draw_img = np.copy(img)\n",
    "    # TODO: Check whether following line is necessary and if better to put in if-statement,\n",
    "    # e.g. if image already is 0 to 1 scale\n",
    "    img = img.astype(np.float32)/255\n",
    "    \n",
    "    # TODO: check whether values of start stop are None and adapt accordingly\n",
    "    img_search = img[y_start_stop[0]:y_start_stop[1], x_start_stop[0]:x_start_stop[1], :]\n",
    "    if cspace != None:\n",
    "        feature_img = cv2.cvtColor(img_search, color_transforms[cspace])\n",
    "    else:\n",
    "        feature_img = img_search\n",
    "        \n",
    "    if scale != 1:\n",
    "        img_shape = feature_img.shape\n",
    "        feature_img = cv2.resize(feature_img, (np.int(img_shape[1]/scale), np.int(img_shape[0]/scale)))\n",
    "    \n",
    "    # TODO: Check if split to channels is necessary\n",
    "    ch1 = feature_img[:,:,0]\n",
    "    ch2 = feature_img[:,:,1]\n",
    "    ch3 = feature_img[:,:,2]\n",
    "    \n",
    "    # Define blocks and steps\n",
    "    n_blocks_x = (ch1.shape[1] // pix_per_cell) - cell_per_block + 1 # TODO: Understand why it is like this and what celss, blocks and windows are\n",
    "    n_blocks_y = (ch1.shape[0] // pix_per_cell) - cell_per_block + 1\n",
    "    n_feat_per_block = orient * np.square(cell_per_block) # TODO: Check if np.square is the right function\n",
    "    n_blocks_per_window = (window // pix_per_cell) - cell_per_block + 1\n",
    "    n_steps_x = (n_blocks_x - n_blocks_per_window) // cells_per_step\n",
    "    n_steps_y = (n_blocks_y - n_blocks_per_window) // cells_per_step\n",
    "    \n",
    "    # Compute individual channel hog features\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    \n",
    "    for step_x in range(n_steps_x):\n",
    "        for step_y in range(n_steps_y):\n",
    "            pos_y = step_y * cells_per_step\n",
    "            pos_x = step_x * cells_per_step\n",
    "            \n",
    "            # Extract hog for this patch\n",
    "            # TODO: Understand why it works like this and what n_blocks_per_windo does here\n",
    "            hog_feat1 = hog1[pos_y:pos_y + n_blocks_per_window, pos_x:pos_x + n_blocks_per_window].ravel() \n",
    "            hog_feat2 = hog2[pos_y:pos_y + n_blocks_per_window, pos_x:pos_x + n_blocks_per_window].ravel()\n",
    "            hog_feat3 = hog3[pos_y:pos_y + n_blocks_per_window, pos_x:pos_x + n_blocks_per_window].ravel()\n",
    "            \n",
    "            hog_features = np.hstack((hog1, hog2, hog3)) # TODO: Check if vstack works and if not why not\n",
    "            \n",
    "            x_left = pos_x * pix_per_cell\n",
    "            y_top = pos_y * pix_per_cell\n",
    "            \n",
    "            # Extract the image patch\n",
    "            sub_img = cv2.resize(feature_img[y_top:y_top+window, x_left:x_left+window])\n",
    "            \n",
    "            # Get color features\n",
    "            spatial_features = bin_spatial(sub_img, size=spatial_size)\n",
    "            hist_features = color_hist(sub_img, nbins=hist_bins)\n",
    "            \n",
    "            # Scale features and make a prediction\n",
    "            # TODO: Check if vstack works\n",
    "            features = X_scaler.transform(np.hstacl((spatial_features, hist_features, hog_features)).reshape(1, -1))\n",
    "            prediction = svc.predict(features)\n",
    "            \n",
    "            # If prediction was car (1)\n",
    "            if prediction == 1:\n",
    "                box_left = np.int(x_left * scale)\n",
    "                box_top = np.int(y_top * scale)\n",
    "                win_draw = np.int(window * scale)\n",
    "                cv2.rectangle(draw_img, (box_left, y_top+y_start_stop[0]), \n",
    "                              (box_left+win_draw, y_top+win_draw+y_start_stop[0]), color, thick)\n",
    "            \n",
    "            return draw_img\n",
    "\n",
    "# SEARCH OVER ALL WINDOWS\n",
    "# Extract features from a single image window\n",
    "def single_img_features(img, color_space=None, spatial_size=(32,32), hist_bins=32, orient=9, pix_per_cell=8,\n",
    "                        cell_per_block=2, hog_channel=0, spatial_feat=True, hist_feat=True, hog_feat=True, \n",
    "                        color_transforms=RGB_COLOR_TRANSFORMS):\n",
    "\"\"\"\n",
    "img: \n",
    "color_space: \n",
    "spatial_size: \n",
    "hist_bins: \n",
    "orient: \n",
    "pix_per_cell: \n",
    "cell_per_block: \n",
    "hog_channel: \n",
    "spatial_feat: \n",
    "hist_feat: \n",
    "hog_feat: \n",
    "color_transforms: \n",
    "return: \n",
    "\"\"\"\n",
    "    # Empty list to receive features\n",
    "    features = []\n",
    "    # Apply color conversion\n",
    "    if color_space != None:\n",
    "        feature_img = cv2.cvtColor(img, color_transforms[color_space])\n",
    "    else:\n",
    "        feature_img = np.copy(img)\n",
    "    # Compute spatial features\n",
    "    if spatial_feat:\n",
    "        spatial_features = bin_spatial(feature_img, size=spatial_size)\n",
    "        features.append(spatial_features)\n",
    "    # Compute histogram features\n",
    "    if hist_feat:\n",
    "        hist_features = color_hist(feature_img, nbins=hist_bins)\n",
    "        features.append(hist_features)\n",
    "    # Compute hog features\n",
    "    if hog_feat:\n",
    "        if hog_channel == \"ALL\":\n",
    "            hog_features = []\n",
    "            for channel in range(feature_img.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_img[:,:,channel], orient, pix_per_cell, cell_per_block,\n",
    "                                                    vis=False, feature_vec=True))\n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_img[:,:,hog_channel], orient, pix_per_cell, cell_per_block, \n",
    "                                            vis=False, feature_vec=True)\n",
    "        features.appen(hog_features)\n",
    "    \n",
    "    return np.concatenate(features)\n",
    "        \n",
    "# OBJECT SEARCH\n",
    "# Search in windows for target objects\n",
    "def search_windows(img, windows, clf, scaler, color_space=None, window_size=(64,64), spatial_size=(32,32), hist_bins=32, \n",
    "                   hist_range(0, 256), orient=9, pix_per_cell=8, cell_per_block=2, hog_channel=0, spatial_feat=True, \n",
    "                   hist_feat=True, hog_feat=True, color_transforms=RGB_COLOR_TRANSFORMS):\n",
    "\"\"\"\n",
    "img:\n",
    "windows:\n",
    "clf:\n",
    "scaler:\n",
    "color_space:\n",
    "window_size:\n",
    "spatial_size:\n",
    "hist_bins:\n",
    "hist_range:\n",
    "orient:\n",
    "pix_per_cell:\n",
    "cell_per_block:\n",
    "hog_channel:\n",
    "spatial_feat:\n",
    "hist_feat:\n",
    "hog_feat:\n",
    "color_transforms:\n",
    "return:\n",
    "\"\"\"\n",
    "    # Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    # Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        # Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0], window[1][0]], window_size) # TODO: Check whether window_size is necessary, what it does and why not just set it to (64,64)\n",
    "        # Extract features for that window\n",
    "        features = single_img_features(test_img, color_space=color_space, spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                                      orient=orient, pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                                      hog_channel=hog_channel, spatial_feat=spatial_feat, hist_feat=hist_feat,\n",
    "                                      hog_feat=hog_feat, color_transforms=color_transforms)\n",
    "        # Scale extracted features\n",
    "        features = scaler.transform(np.array(features).reshape(1, -1)) # TODO: Check if vstack works\n",
    "        # Predict the class\n",
    "        prediction = clf.predict(features)\n",
    "        # If prediction is car (1) save window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "        \n",
    "        return on_windows\n",
    "    \n",
    "# HEATMAP\n",
    "# Add heat values to heatmap for pixels within bounding box\n",
    "def add_heat(heatmap, bbox_list):\n",
    "\"\"\"\n",
    "heatmap: image with pixel values representing heat\n",
    "bbox_list: list of bounding box coordinates as tuples\n",
    "return: heatmap with updated heat values\n",
    "\"\"\"\n",
    "    # Iterate through list of bounding boxes\n",
    "    for box in bbox_list:\n",
    "        # Add 1 for all pixels inside each box\n",
    "        # Assuming each box takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "        \n",
    "    return heatmap\n",
    "\n",
    "# Zero values below threshold\n",
    "def apply_threshold(heatmap, threshold):\n",
    "\"\"\"\n",
    "heatmap: image with pixel values representing heat\n",
    "threshold: threshold value for zeroing values\n",
    "return: thresholded heatmap\n",
    "\"\"\"\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    \n",
    "    return heatmap\n",
    "\n",
    "# Draw bounding boxes around identified objects\n",
    "def draw_labeled_bboxes(img, labels, color=(255, 0, 0), thick=6):\n",
    "\"\"\"\n",
    "img: image to draw on\n",
    "labels: \n",
    "color: color of the bounding boxes\n",
    "thick: line thickness of the bounding boxes\n",
    "return: image with bounding boxes\n",
    "\"\"\"\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, len(labels) + 1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzero_y = np.arary(nonzero[0])\n",
    "        nonzero_x = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzero_x), np.min(nonzero_y)), (np.max(nonzero_x), np.max(nonzero_y)))\n",
    "        # Draw box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], color, thick)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As an optional challenge** Once you have a working pipeline for vehicle detection, add in your lane-finding algorithm from the last project to do simultaneous lane-finding and vehicle detection!\n",
    "\n",
    "**If you're feeling ambitious** (also totally optional though), don't stop there!  We encourage you to go out and take video of your own, and show us how you would implement this project on a new video!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Plan for implementation:\n",
    "* Decide on feature combination (color and gradient)\n",
    "* Train classifier\n",
    "* Sliding window for detection or maybe hog subsampling !?\n",
    "\n",
    "#### Proposal for Prediction Pipeline\n",
    "The pipeline might vary for CNN classifier, but for decision trees, naive bayes, support vector machines, and neural networks (on feature vectors not image information) the pipeline should follow these steps:\n",
    "* Read in image\n",
    "* Extract features\n",
    "* Normalize feature vector\n",
    "* Feed into classifier\n",
    "\n",
    "#### Proposal for Training Pipeline\n",
    "* Read in images\n",
    "* Extract features\n",
    "* Normalize features\n",
    "* Train classifier\n",
    "\n",
    "\n",
    "#### To be explored\n",
    "* Make use of the sklearn pipeline framework similar to the traffic sign project example\n",
    "* Have different parameter configurations explored\n",
    "* Have a look at SURF and ? (computer vision in Sweden) features\n",
    "* Streaming video in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yellow_output = 'test_videos_output/solidYellowLeft.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip2 = VideoFileClip('test_videos/solidYellowLeft.mp4').subclip(0,5)\n",
    "clip2 = VideoFileClip('test_videos/solidYellowLeft.mp4')\n",
    "yellow_clip = clip2.fl_image(process_image)\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(yellow_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"test_video.mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"test_video.mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ud_sdcnd]",
   "language": "python",
   "name": "conda-env-ud_sdcnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
